{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66386297-8e87-43e6-8ce5-00b9af4f5770",
   "metadata": {},
   "source": [
    "# DNN-Based DDoS Anomaly Detection in the Network Data Plane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21b726-813f-46c0-b9aa-1c0a93eb6f03",
   "metadata": {},
   "source": [
    "In this 4-part notebook series, we show how a quantized neural network (QNN) can be trained to classify packets as belonging to DDoS (malicious) or regular (benign) network traffic flows. The model is trained with quantized weights and activations, and we use the [Brevitas](https://github.com/Xilinx/brevitas) framework to train the QNN. The model is then converted into an FPGA-friendly RTL implementation for high-throughput inference, which can be integrated with a packet-processing pipeline in the network data plane.\n",
    "\n",
    "This notebook series is composed of 4 parts. Below is a brief summary of what each part covers.\n",
    "\n",
    "[Part 1](./1-train.ipynb): How to use Brevitas to train a quantized neural network for our target application, which is classifying packets as belonging to malicious/DDoS or benign/normal network traffic flows. The output trained model at the end of this part is a pure software implementation, i.e. it cannot be converted to a custom RTL FINN model to run on an FPGA just yet.\n",
    "\n",
    "[Part 2](./2-prepare.ipynb): This notebook focuses on taking the output software model from the previous part and preparing it for hardware-friendly implementation using the FINN framework. The notebook describes the steps taken to \"surgery\" the software model in order for hardware generation via FINN. We also verify that all the changes made to the software model in this notebook DO NOT affect the output predictions in the \"surgeried\" model.\n",
    "\n",
    "[Part 3](./3-build.ipynb): In this notebook, we use the FINN framework to build the custom RTL accelerator for our target model. FINN can generate a variety of RTL accelerators, and this notebook covers some build configuration parameters that influence these outputs.\n",
    "\n",
    "[Part 4](./4-verify.ipynb): The generated hardware is simulated using cycle-accurate RTL simulation tools, and its outputs are compared against the original software-only model trained in part one. The output model from this step is now ready to be integrated into a larger FPGA design, which in this context is a packet-processing network data plane pipeline designed for identifying anomalous DDoS flows from benign flows.\n",
    "\n",
    "This tutorial series is a supplement to our demo paper presented at EuroP4 2023 workshop, titled [Enabling DNN Inference in the Network Data Plane](https://dl.acm.org/doi/10.1145/3630047.3630191). You can cite our work using the following BibTeX snippet:\n",
    "\n",
    "```\n",
    "@inproceedings{siddhartha2023enabling,\n",
    "  title={Enabling DNN Inference in the Network Data Plane},\n",
    "  author={Siddhartha and Tan, Justin and Bansal, Rajesh and Chee Cheun, Huang and Tokusashi, Yuta and Yew Kwan, Chong and Javaid, Haris and Baldi, Mario},\n",
    "  booktitle={Proceedings of the 6th on European P4 Workshop},\n",
    "  pages={65--68},\n",
    "  year={2023}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ebd5c-8bc6-4b50-be59-946a96a19cf2",
   "metadata": {},
   "source": [
    "# Part 3: Building the FINN hardware accelerator\n",
    "\n",
    "In this part, we will take the FINN-ONNX model prepared in [Part Two](./2-prepare.ipynb) of this example series and convert it into an RTL implementation using the FINN framework. Details on how the FINN tooling works can be found in [this notebook](https://github.com/Xilinx/finn/blob/v0.10/notebooks/end2end_example/cybersecurity/3-build-accelerator-with-finn.ipynb). This notebook focuses more on detailing one of the many approaches that was used to generate the FINN accelerator in this example.\n",
    "\n",
    "Let's first start as usual with the house-keeping.\n",
    "\n",
    "## House-Keeping\n",
    "\n",
    "We will import necessary libraries/packages and declare global constants for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df27e0f-a392-4166-a0ae-1b6ac7a09465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import onnx\n",
    "import json\n",
    "from os.path import join\n",
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "# Path to this end-to-end example's directory\n",
    "EXAMPLE_DIR = join(os.environ['FINN_ROOT'], \"notebooks/end2end_example/ddos-anomaly-detector\")\n",
    "\n",
    "# Path to build directory from part two\n",
    "BUILD_DIR_P2 = join(EXAMPLE_DIR, \"build\", \"part_02\")\n",
    "\n",
    "# Path to build directory to write outputs from this notebook to\n",
    "BUILD_DIR = join(EXAMPLE_DIR, \"build\", \"part_03\")\n",
    "os.makedirs(BUILD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1294bb-cb85-434c-bcbd-aa11e2591709",
   "metadata": {},
   "source": [
    "For this notebook, we only need the FINN-ONNX model prepared in part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84a86e9-1104-43b4-b954-9ac613aca217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/home/sids/workspace/project_find_ml/finn/notebooks/end2end_example/ddos-anomaly-detector/build/part_02/ready-for-finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbcd83240a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to FINN-ONNX model from part two\n",
    "model_for_export_fpath = join(BUILD_DIR_P2, \"ready-for-finn.onnx\")\n",
    "\n",
    "# View the model in Netron\n",
    "showInNetron(model_for_export_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3e6722-40d6-4d37-90b0-237064bac615",
   "metadata": {},
   "source": [
    "## The FINN build process\n",
    "\n",
    "The FINN build process is centered around the `build_dataflow` tool, which enables programmers to specify a `build_config` as a python dictionary (`dict`) of configuration parameters that guide the compilation process. There are various build outputs that can be generated, ranging from quick estimates to complete FPGA toolflow invocations. In this notebook, we demonstrate one particular sequence of steps to building a FINN accelerator for enabling data plane AI applications. More details can be found in [this notebook](https://github.com/Xilinx/finn/blob/v0.10/notebooks/end2end_example/cybersecurity/3-build-accelerator-with-finn.ipynb), [this examples repository](https://github.com/Xilinx/finn-examples/tree/main), [official documentation](https://finn.readthedocs.io/en/latest/end_to_end_flow.html), and [advanced builder notebooks](https://github.com/Xilinx/finn/tree/v0.10/notebooks/advanced).\n",
    "\n",
    "### Estimating FINN hardware accelerator utilization and performance\n",
    "\n",
    "FINN provides a fast analytical flow that enable programmers to get rough estimates of resource utilization and achievable performance for the target design. This step does not invoke any synthesis (including HLS), and hence, only takes seconds to complete. We use this flow to also produce a `auto_folding_config.json` file, which we then modify manually to produce FINN output more optimized for our use-case. The generated `auto_folding_config.json` gives us a reasonable base optimization configuration to iterate on, and is a recommended approach for experimenting with FINN RTL outputs.\n",
    "\n",
    "We start by declaring path to directory where all output for these estimates will be generated. Note that we also delete any existing builds, since re-generating estimation builds only takes a few seconds. You should, however, backup any existing builds manually beforehand if you need them in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "113f116a-9742-48cd-a5b0-625131cae4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous run results deleted!\n"
     ]
    }
   ],
   "source": [
    "# directory to generate estimates flow outputs to\n",
    "estimates_output_dir = join(BUILD_DIR, \"output_estimates\")\n",
    "\n",
    "# Delete previous run results if they exist\n",
    "if os.path.exists(estimates_output_dir):\n",
    "    shutil.rmtree(estimates_output_dir, ignore_errors=True)\n",
    "    print(\"Previous run results deleted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba1a0f2-5cd5-4ddf-9ae7-bf6b77468482",
   "metadata": {},
   "source": [
    "Next, we declare the build config that will be used in the FINN build process. Note that we are setting a very high target FPS (or inference rate) of 250M inferences/second. This is an aggressive target to meet line-rate inference rates for our target packet-rate in our FPGA NIC, i.e. we want to be able to support inference on every packet entering our networking pipeline on the FPGA. The target clock is 4ns and the accelerator is targeted at the Alveo U250 FPGA card.\n",
    "\n",
    "Choosing a value for `mvau_wwidth_max` is a little more complicated, as it controls the maximum width of the per-PE MVAU (Matrix Vector Activate Unit) stream. Giving this a larger value allows the tool to explore more parallel but larger design points to reach `target_fps`, and should be set to something large if targeting full unfolding or very high performance. For our design, we set this to 300 as it delivers a design capable of meeting our `target_fps` of 250M inferences/sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b25e35-5aa1-42a7-afb4-cd225b5e3a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_estimates = build.DataflowBuildConfig(\n",
    "    output_dir          = estimates_output_dir,\n",
    "    mvau_wwidth_max     = 300,\n",
    "    target_fps          = 250000000,                # 250M inf/sec\n",
    "    synth_clk_period_ns = 4.0,                      # 250MHz\n",
    "    fpga_part           = \"xcu250-figd2104-2L-e\",   # Alveo U250\n",
    "    steps               = build_cfg.estimate_only_dataflow_steps,\n",
    "    generate_outputs    = [\n",
    "        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8491a2-2522-490e-92b9-a8a73b126394",
   "metadata": {},
   "source": [
    "We can now run and time this build config by passing it as an argument to the FINN build tool, along with the path to the FINN-ONNX model we would like to convert to RTL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74384659-a330-458f-a649-0540ffda02c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataflow accelerator from /home/sids/workspace/project_find_ml/finn/notebooks/end2end_example/ddos-anomaly-detector/build/part_02/ready-for-finn.onnx\n",
      "Intermediate outputs will be generated in /tmp/finn_dev_sids\n",
      "Final outputs will be generated in /home/sids/workspace/project_find_ml/finn/notebooks/end2end_example/ddos-anomaly-detector/build/part_03/output_estimates\n",
      "Build log is at /home/sids/workspace/project_find_ml/finn/notebooks/end2end_example/ddos-anomaly-detector/build/part_03/output_estimates/build_dataflow.log\n",
      "Running step: step_qonnx_to_finn [1/10]\n",
      "Running step: step_tidy_up [2/10]\n",
      "Running step: step_streamline [3/10]\n",
      "Running step: step_convert_to_hw [4/10]\n",
      "Running step: step_create_dataflow_partition [5/10]\n",
      "Running step: step_specialize_layers [6/10]\n",
      "Running step: step_target_fps_parallelization [7/10]\n",
      "Running step: step_apply_folding_config [8/10]\n",
      "Running step: step_minimize_bit_width [9/10]\n",
      "Running step: step_generate_estimate_reports [10/10]\n",
      "Completed successfully\n",
      "CPU times: user 579 ms, sys: 0 ns, total: 579 ms\n",
      "Wall time: 617 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "build.build_dataflow_cfg(model_for_export_fpath, cfg_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62691bb9-ff21-467c-ac49-ccc03c22f593",
   "metadata": {},
   "source": [
    "Make sure that expected reports were generated and print out some of the estimates on performance and resource utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "701750fc-ddf7-4bb6-b7e6-3af79aa8540e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"critical_path_cycles\": 3,\n",
      "  \"max_cycles\": 1,\n",
      "  \"max_cycles_node_name\": \"MVAU_hls_0\",\n",
      "  \"estimated_throughput_fps\": 250000000.0,\n",
      "  \"estimated_latency_ns\": 12.0\n",
      "}{\n",
      "  \"MVAU_hls_0\": 1,\n",
      "  \"MVAU_hls_1\": 1,\n",
      "  \"MVAU_hls_2\": 1\n",
      "}{\n",
      "  \"MVAU_hls_0\": {\n",
      "    \"BRAM_18K\": 228,\n",
      "    \"BRAM_efficiency\": 0.001949317738791423,\n",
      "    \"LUT\": 40991,\n",
      "    \"URAM\": 0,\n",
      "    \"URAM_efficiency\": 1,\n",
      "    \"DSP\": 0\n",
      "  },\n",
      "  \"MVAU_hls_1\": {\n",
      "    \"BRAM_18K\": 57,\n",
      "    \"BRAM_efficiency\": 0.001949317738791423,\n",
      "    \"LUT\": 13922,\n",
      "    \"URAM\": 0,\n",
      "    \"URAM_efficiency\": 1,\n",
      "    \"DSP\": 0\n",
      "  },\n",
      "  \"MVAU_hls_2\": {\n",
      "    \"BRAM_18K\": 2,\n",
      "    \"BRAM_efficiency\": 0.001736111111111111,\n",
      "    \"LUT\": 725,\n",
      "    \"URAM\": 0,\n",
      "    \"URAM_efficiency\": 1,\n",
      "    \"DSP\": 0\n",
      "  },\n",
      "  \"total\": {\n",
      "    \"BRAM_18K\": 287.0,\n",
      "    \"LUT\": 55638.0,\n",
      "    \"URAM\": 0.0,\n",
      "    \"DSP\": 0.0\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "assert os.path.exists(join(estimates_output_dir, \"report/estimate_network_performance.json\"))\n",
    "! cat {estimates_output_dir}/report/estimate_network_performance.json\n",
    "! cat {estimates_output_dir}/report/estimate_layer_cycles.json\n",
    "! cat {estimates_output_dir}/report/estimate_layer_resources.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd2327d-c0c4-407c-b7dd-2388abd85fd5",
   "metadata": {},
   "source": [
    "Note that the estimated throughput is 250M inferences/sec, which is achieved by fully unfolding all of the MVAU layers in the model. This produces layers that only take a single clock cycle to evaluate the result, which may cause timing errors during synthesis due to various factors such as layers are too large, model is too deep, etc.\n",
    "\n",
    "For now, our focus is to take the `auto_folding_config.json` produced by this previous step and observe the configuration parameters estimated by the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e381068-75ce-4958-b53d-88942b63a203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Defaults\": {},\n",
      "    \"MVAU_hls_0\": {\n",
      "        \"PE\": 32,\n",
      "        \"SIMD\": 128,\n",
      "        \"ram_style\": \"auto\",\n",
      "        \"resType\": \"auto\",\n",
      "        \"mem_mode\": \"internal_decoupled\",\n",
      "        \"runtime_writeable_weights\": 0\n",
      "    },\n",
      "    \"MVAU_hls_1\": {\n",
      "        \"PE\": 32,\n",
      "        \"SIMD\": 32,\n",
      "        \"ram_style\": \"auto\",\n",
      "        \"resType\": \"auto\",\n",
      "        \"mem_mode\": \"internal_decoupled\",\n",
      "        \"runtime_writeable_weights\": 0\n",
      "    },\n",
      "    \"MVAU_hls_2\": {\n",
      "        \"PE\": 1,\n",
      "        \"SIMD\": 32,\n",
      "        \"ram_style\": \"auto\",\n",
      "        \"resType\": \"auto\",\n",
      "        \"mem_mode\": \"internal_decoupled\",\n",
      "        \"runtime_writeable_weights\": 0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "auto_folding_config_file = join(estimates_output_dir, \"auto_folding_config.json\")\n",
    "with open(auto_folding_config_file, \"r\") as fp:\n",
    "    auto_folding_config = json.load(fp)\n",
    "print(json.dumps(auto_folding_config, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d531cfdf-49b7-421b-a799-97919b4c8802",
   "metadata": {},
   "source": [
    "Note how `PE` and `SIMD` parameters are selected for each of the MVAU layers, and how they correspond to the number of neurons and inputs to each layer. More details about `PE` and `SIMD` parameters can be found [in this documentation page](https://finn-dev.readthedocs.io/en/latest/internals.html#constraints-to-folding-factors-per-layer).\n",
    "\n",
    "The only parameter we would like to change in this configuration is `mem_mode`. Instead of using the default `internal_decoupled`, we switch it to `internal_embedded` for a smaller resource footprint. In `internal_embedded`, weights are \"baked\" into the MVAU HLS module, and the tool is free to place the weight matrices whichever way it sees fit. In the `internal_decoupled` strategy, an RTL-based weight streamer module is used to stream weights into the HLS layers, incurring some slight resource overheads for the control circuitry. Choosing `_embedded` over `_decoupled`, however, may not be advantageous all the time, given that weight memory allocation is left completely to the HLS tool, which is not always good at inferring the optimal primitives. More details can be found [in the documentation here](https://finn.readthedocs.io/en/latest/internals.html#hls-variant-of-matrixvectoractivation-mem-mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efa6998d-e8f6-4513-8938-4363aa3c0e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous run results deleted!\n"
     ]
    }
   ],
   "source": [
    "# Delete previous run results since we are going to re-generate\n",
    "# them with our own folding_config.json\n",
    "if os.path.exists(estimates_output_dir):\n",
    "    shutil.rmtree(estimates_output_dir, ignore_errors=True)\n",
    "    print(\"Previous run results deleted!\")\n",
    "\n",
    "# change mem_mode to 'internal_embedded' for each of the layers in the config\n",
    "for key in auto_folding_config:\n",
    "    if key == \"Defaults\":\n",
    "        continue\n",
    "    auto_folding_config[key][\"mem_mode\"] = \"internal_embedded\"\n",
    "\n",
    "# write out our modified folding config\n",
    "my_folding_config_file = join(BUILD_DIR, \"my_folding_config.json\")\n",
    "os.makedirs(estimates_output_dir, exist_ok=True)\n",
    "with open(my_folding_config_file, \"w\") as fp:\n",
    "    json.dump(auto_folding_config, fp)\n",
    "\n",
    "# Alveo U250\n",
    "fpga_part = \"xcu250-figd2104-2L-e\"\n",
    "\n",
    "# re-declare the cfg_estimates with an extra argument for folding_config_file\n",
    "cfg_estimates = build.DataflowBuildConfig(\n",
    "    output_dir          = estimates_output_dir,\n",
    "    mvau_wwidth_max     = 300,\n",
    "    target_fps          = 250000000,                # 250M inf/sec\n",
    "    synth_clk_period_ns = 4.0,                      # 250MHz\n",
    "    fpga_part           = \"xcu250-figd2104-2L-e\",   # Alveo U250\n",
    "    steps               = build_cfg.estimate_only_dataflow_steps,\n",
    "    folding_config_file = my_folding_config_file,   # our own custom folding_config.json\n",
    "    generate_outputs    = [\n",
    "        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6707d481-b6b6-4932-8fc8-af698f34f2a6",
   "metadata": {},
   "source": [
    "Let's re-run and time this new build config, and assert that the expected reports were generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9acec436-2498-4054-813e-f74fb2a92038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataflow accelerator from /home/sids/workspace/project_find_ml/finn/notebooks/end2end_example/ddos-anomaly-detector/build/part_02/ready-for-finn.onnx\n",
      "Intermediate outputs will be generated in /tmp/finn_dev_sids\n",
      "Final outputs will be generated in /home/sids/workspace/project_find_ml/finn/notebooks/end2end_example/ddos-anomaly-detector/build/part_03/output_estimates\n",
      "Build log is at /home/sids/workspace/project_find_ml/finn/notebooks/end2end_example/ddos-anomaly-detector/build/part_03/output_estimates/build_dataflow.log\n",
      "Running step: step_qonnx_to_finn [1/10]\n",
      "Running step: step_tidy_up [2/10]\n",
      "Running step: step_streamline [3/10]\n",
      "Running step: step_convert_to_hw [4/10]\n",
      "Running step: step_create_dataflow_partition [5/10]\n",
      "Running step: step_specialize_layers [6/10]\n",
      "Running step: step_target_fps_parallelization [7/10]\n",
      "Running step: step_apply_folding_config [8/10]\n",
      "Running step: step_minimize_bit_width [9/10]\n",
      "Running step: step_generate_estimate_reports [10/10]\n",
      "Completed successfully\n",
      "{\n",
      "  \"critical_path_cycles\": 3,\n",
      "  \"max_cycles\": 1,\n",
      "  \"max_cycles_node_name\": \"MVAU_hls_0\",\n",
      "  \"estimated_throughput_fps\": 250000000.0,\n",
      "  \"estimated_latency_ns\": 12.0\n",
      "}{\n",
      "  \"MVAU_hls_0\": 1,\n",
      "  \"MVAU_hls_1\": 1,\n",
      "  \"MVAU_hls_2\": 1\n",
      "}{\n",
      "  \"MVAU_hls_0\": {\n",
      "    \"BRAM_18K\": 0,\n",
      "    \"BRAM_efficiency\": 1,\n",
      "    \"LUT\": 49183,\n",
      "    \"URAM\": 0,\n",
      "    \"URAM_efficiency\": 1,\n",
      "    \"DSP\": 0\n",
      "  },\n",
      "  \"MVAU_hls_1\": {\n",
      "    \"BRAM_18K\": 0,\n",
      "    \"BRAM_efficiency\": 1,\n",
      "    \"LUT\": 15970,\n",
      "    \"URAM\": 0,\n",
      "    \"URAM_efficiency\": 1,\n",
      "    \"DSP\": 0\n",
      "  },\n",
      "  \"MVAU_hls_2\": {\n",
      "    \"BRAM_18K\": 0,\n",
      "    \"BRAM_efficiency\": 1,\n",
      "    \"LUT\": 789,\n",
      "    \"URAM\": 0,\n",
      "    \"URAM_efficiency\": 1,\n",
      "    \"DSP\": 0\n",
      "  },\n",
      "  \"total\": {\n",
      "    \"BRAM_18K\": 0.0,\n",
      "    \"LUT\": 65942.0,\n",
      "    \"URAM\": 0.0,\n",
      "    \"DSP\": 0.0\n",
      "  }\n",
      "}CPU times: user 553 ms, sys: 44.3 ms, total: 597 ms\n",
      "Wall time: 1.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "build.build_dataflow_cfg(model_for_export_fpath, cfg_estimates)\n",
    "assert os.path.exists(join(estimates_output_dir, \"report/estimate_network_performance.json\"))\n",
    "! cat {estimates_output_dir}/report/estimate_network_performance.json\n",
    "! cat {estimates_output_dir}/report/estimate_layer_cycles.json\n",
    "! cat {estimates_output_dir}/report/estimate_layer_resources.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03816a93-8744-4fda-a56e-21cc9deb700c",
   "metadata": {},
   "source": [
    "Note that no performance-related numbers (i.e. latency, throughput) should have changed, but the resource utilization of various FPGA primitives would have changed. This is because the weight matrices are now included in the HLS layers and hence the analytical model can only estimate the LUT utilization of these layers now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3e46e9-c16a-4677-be2f-8a6461b098b2",
   "metadata": {},
   "source": [
    "### Building the FINN Stitched IP\n",
    "\n",
    "Let's start by writing our build config. We want to generate 3 types of outputs:\n",
    "    - `STITCHED_IP`: Generate a stitched Vivado IP block design that can be integrated with our broader packet-processing pipeline\n",
    "    - `RTLSIM_PERFORMANCE`: Run RTL simulation to estimate throughput and latency performance of the design. Note that we also provide an additional `rtlsim_batch_size` parameter, which sets the number of inputs that are fed in to estimate the throughput performance. By increasing it from default 1, we can get a better throughput estimate from the tool.\n",
    "    - `OOC_SYNTH`: Runs out-of-context synthesis for the stitched IP, which is useful for getting post-synthesis resource counts and achievable clock frequency.\n",
    "\n",
    "Note that we provide our customized `folding_config.json` file, and the `steps = build_cfg.estimate_only_dataflow_steps` line has been removed.\n",
    "\n",
    "**PLEASE READ:** Finally, since this build runs through synthesis and implementation, this build can take a long time (minutes to hours) depending on the size of your network. Hence, we may not wish to clear the generated output, especially if we only want to run the follow-up steps in this notebook with an existing build. To clear an existing build (if it exists) and (re-)build the stitched IP, set `rtl_cleanup` to `True` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a406f91-d49e-4d2d-8db1-cf1b397fbc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to generate IP flow outputs to\n",
    "rtl_output_dir = join(BUILD_DIR, \"output_rtl\")\n",
    "\n",
    "# NOTE: Set to True if you want to clear output and re-generate the stitched IP\n",
    "rtl_cleanup = False\n",
    "\n",
    "# Delete previous run results if exist\n",
    "if rtl_cleanup:\n",
    "    if os.path.exists(rtl_output_dir):\n",
    "        shutil.rmtree(rtl_output_dir)\n",
    "        print(\"Previous run results deleted!\")\n",
    "\n",
    "    cfg_stitched_ip = build.DataflowBuildConfig(\n",
    "        output_dir          = rtl_output_dir,\n",
    "        mvau_wwidth_max     = 300,\n",
    "        target_fps          = 250000000,                # 250M inf/sec\n",
    "        synth_clk_period_ns = 4.0,                      # 250MHz\n",
    "        fpga_part           = \"xcu250-figd2104-2L-e\",   # Alveo U250\n",
    "        folding_config_file = my_folding_config_file,   # our own custom folding_config.json\n",
    "        rtlsim_batch_size   = 4,                        # >1 for better throughput estimation\n",
    "        generate_outputs=[\n",
    "            build_cfg.DataflowOutputType.STITCHED_IP,\n",
    "            build_cfg.DataflowOutputType.RTLSIM_PERFORMANCE,\n",
    "            build_cfg.DataflowOutputType.OOC_SYNTH,\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425c3a2f-ab30-423a-9dc2-25413d16d686",
   "metadata": {},
   "source": [
    "Let's run and time the build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "152c336c-3c16-448b-9ccf-98e8a18a6551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataflow accelerator from /home/sids/workspace/project_find_ml/finn/notebooks/end2end_example/ddos-anomaly-detector/build/part_02/ready-for-finn.onnx\n",
      "Intermediate outputs will be generated in /tmp/finn_dev_sids\n",
      "Final outputs will be generated in /home/sids/workspace/project_find_ml/finn/notebooks/end2end_example/ddos-anomaly-detector/build/part_03/output_rtl\n",
      "Build log is at /home/sids/workspace/project_find_ml/finn/notebooks/end2end_example/ddos-anomaly-detector/build/part_03/output_rtl/build_dataflow.log\n",
      "Running step: step_qonnx_to_finn [1/19]\n",
      "Running step: step_tidy_up [2/19]\n",
      "Running step: step_streamline [3/19]\n",
      "Running step: step_convert_to_hw [4/19]\n",
      "Running step: step_create_dataflow_partition [5/19]\n",
      "Running step: step_specialize_layers [6/19]\n",
      "Running step: step_target_fps_parallelization [7/19]\n",
      "Running step: step_apply_folding_config [8/19]\n",
      "Running step: step_minimize_bit_width [9/19]\n",
      "Running step: step_generate_estimate_reports [10/19]\n",
      "Running step: step_hw_codegen [11/19]\n",
      "Running step: step_hw_ipgen [12/19]\n",
      "Running step: step_set_fifo_depths [13/19]\n",
      "Running step: step_create_stitched_ip [14/19]\n",
      "Running step: step_measure_rtlsim_performance [15/19]\n",
      "Running step: step_out_of_context_synthesis [16/19]\n",
      "Running step: step_synthesize_bitfile [17/19]\n",
      "Running step: step_make_pynq_driver [18/19]\n",
      "Running step: step_deployment_package [19/19]\n",
      "Completed successfully\n",
      "CPU times: user 1.23 s, sys: 189 ms, total: 1.42 s\n",
      "Wall time: 14min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if rtl_cleanup:\n",
    "    build.build_dataflow_cfg(model_for_export_fpath, cfg_stitched_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c85397-6af7-41b2-ae78-dba0b3cf86e7",
   "metadata": {},
   "source": [
    "Let's make sure all the expected reports were generated, and take a peek into the synthesis results of the stitched IP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaa9fae5-6d70-4685-bbea-022180883853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"vivado_proj_folder\": \"/tmp/finn_dev_sids/synth_out_of_context_tql7g45a/results_finn_design_wrapper\",\n",
      "  \"LUT\": 3451.0,\n",
      "  \"LUTRAM\": 0.0,\n",
      "  \"FF\": 3658.0,\n",
      "  \"DSP\": 0.0,\n",
      "  \"BRAM\": 0.0,\n",
      "  \"BRAM_18K\": 0.0,\n",
      "  \"BRAM_36K\": 0.0,\n",
      "  \"URAM\": 0.0,\n",
      "  \"Carry\": 1.0,\n",
      "  \"WNS\": 1.288,\n",
      "  \"Delay\": 1.288,\n",
      "  \"vivado_version\": 0,\n",
      "  \"vivado_build_no\": 3788287.0,\n",
      "  \"\": 0,\n",
      "  \"fmax_mhz\": 368.7315634218289,\n",
      "  \"estimated_throughput_fps\": 368731563.4218289\n",
      "}"
     ]
    }
   ],
   "source": [
    "assert os.path.exists(join(rtl_output_dir, \"report/ooc_synth_and_timing.json\"))\n",
    "assert os.path.exists(join(rtl_output_dir, \"report/rtlsim_performance.json\"))\n",
    "assert os.path.exists(join(rtl_output_dir, \"final_hw_config.json\"))\n",
    "! cat {rtl_output_dir}/report/ooc_synth_and_timing.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36814fca-b795-467b-892e-95074d2d5ebd",
   "metadata": {},
   "source": [
    "You should see that the design meets timing (positive WNS) and the estimated throughput is higher than our desired 250M inferences/sec target. This is good news, and it means we can move onto our final step in this notebook series. In [Part 4](./4-verify.ipynb), we do the final verification of this generated IP by doing a cycle-accurate RTL simulation and comparing it against the software model outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
