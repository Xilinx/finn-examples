{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66386297-8e87-43e6-8ce5-00b9af4f5770",
   "metadata": {},
   "source": [
    "# DNN-Based DDoS Anomaly Detection in the Network Data Plane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21b726-813f-46c0-b9aa-1c0a93eb6f03",
   "metadata": {},
   "source": [
    "In this 4-part notebook series, we show how a quantized neural network (QNN) can be trained to classify packets as belonging to DDoS (malicious) or regular (benign) network traffic flows. The model is trained with quantized weights and activations, and we use the [Brevitas](https://github.com/Xilinx/brevitas) framework to train the QNN. The model is then converted into an FPGA-friendly RTL implementation for high-throughput inference, which can be integrated with a packet-processing pipeline in the network data plane.\n",
    "\n",
    "This notebook series is composed of 4 parts. Below is a brief summary of what each part covers.\n",
    "\n",
    "[Part 1](./1-train.ipynb): How to use Brevitas to train a quantized neural network for our target application, which is classifying packets as belonging to malicious/DDoS or benign/normal network traffic flows. The output trained model at the end of this part is a pure software implementation, i.e. it cannot be converted to a custom RTL FINN model to run on an FPGA just yet.\n",
    "\n",
    "[Part 2](./2-prepare.ipynb): This notebook focuses on taking the output software model from the previous part and preparing it for hardware-friendly implementation using the FINN framework. The notebook describes the steps taken to \"surgery\" the software model in order for hardware generation via FINN. We also verify that all the changes made to the software model in this notebook DO NOT affect the output predictions in the \"surgeried\" model.\n",
    "\n",
    "[Part 3](./3-build.ipynb): In this notebook, we use the FINN framework to build the custom RTL accelerator for our target model. FINN can generate a variety of RTL accelerators, and this notebook covers some build configuration parameters that influence these outputs.\n",
    "\n",
    "[Part 4](./4-verify.ipynb): The generated hardware is simulated using cycle-accurate RTL simulation tools, and its outputs are compared against the original software-only model trained in part one. The output model from this step is now ready to be integrated into a larger FPGA design, which in this context is a packet-processing network data plane pipeline designed for identifying anomalous DDoS flows from benign flows.\n",
    "\n",
    "This tutorial series is a supplement to our demo paper presented at EuroP4 2023 workshop, titled [Enabling DNN Inference in the Network Data Plane](https://dl.acm.org/doi/10.1145/3630047.3630191). You can cite our work using the following BibTeX snippet:\n",
    "\n",
    "```\n",
    "@inproceedings{siddhartha2023enabling,\n",
    "  title={Enabling DNN Inference in the Network Data Plane},\n",
    "  author={Siddhartha and Tan, Justin and Bansal, Rajesh and Chee Cheun, Huang and Tokusashi, Yuta and Yew Kwan, Chong and Javaid, Haris and Baldi, Mario},\n",
    "  booktitle={Proceedings of the 6th on European P4 Workshop},\n",
    "  pages={65--68},\n",
    "  year={2023}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ebd5c-8bc6-4b50-be59-946a96a19cf2",
   "metadata": {},
   "source": [
    "# Part 2: Perform surgery on software model to prepare it for hardware acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c11bf13-3e1f-46fd-b7cb-07e3e18d2262",
   "metadata": {},
   "source": [
    "In this part, we will take the software trained model using Brevitas from the previous notebook ([Part One](./1-train.ipynb)) and perform model \"surgery\" on it in order to prepare it for hardware generation using the FINN framework. This is a common preprocessing step that needs to be carried out before going through the FINN model generation process. Depending on the model that was trained in part one, there may be a need to customize some of these steps for optimal results.\n",
    "\n",
    "### House-keeping\n",
    "\n",
    "Let's first get started with some house-keeping; we will import necessary libraries/packages and declare global constants for this notebook, similar to the house-keeping in part one.\n",
    "\n",
    "Quick note: **always import onnx before torch**. This is a workaround for a [known bug](https://github.com/onnx/onnx/issues/2394)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f800095-44ca-48ab-96bf-db6493cf21d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import onnx\n",
    "import json\n",
    "from os.path import join\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from brevitas.nn import QuantLinear, QuantReLU, QuantIdentity\n",
    "from brevitas.export import export_qonnx\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "from finn.util.visualization import showInNetron\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.core.datatype import DataType\n",
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "\n",
    "from utils.common import bcolors\n",
    "from utils.dataset import CICIDS2017_PerPacket\n",
    "from utils.train_test import test, verify, verify_onnx\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Path to this end-to-end example's directory\n",
    "EXAMPLE_DIR = join(os.environ['FINN_ROOT'], \"notebooks/end2end_example/ddos-anomaly-detector\")\n",
    "\n",
    "# Path to build directory from part one\n",
    "BUILD_DIR_P1 = join(EXAMPLE_DIR, \"build\", \"part_01\")\n",
    "\n",
    "# Path to build directory to write outputs from this notebook to\n",
    "BUILD_DIR = join(EXAMPLE_DIR, \"build\", \"part_02\")\n",
    "os.makedirs(BUILD_DIR, exist_ok=True)\n",
    "\n",
    "# Path to where datasets are stored\n",
    "DATASET_DIR = join(EXAMPLE_DIR, \"data\")\n",
    "\n",
    "# get the target device to run model on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Target device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af8ec2-c5ff-43df-afeb-494dc9290aae",
   "metadata": {},
   "source": [
    "There are two additional house-keeping steps for this notebook: (i) load the binarized test set for verification purposes, and (ii) load the trained model from part one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd1cf14-c50c-4001-bb86-237fb3cf2199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIC-IDS2017 per-packet-level dataset\n",
      "Loaded dataset of length = 130777\n",
      "Dataset statistics: 67375/130777 (51.52% TRUE labels)\n",
      "Dataset metadata: {\n",
      "    \"total_in_bitwidth\": 128,\n",
      "    \"ordering\": [\n",
      "        [\n",
      "            \"total_bytes\",\n",
      "            32\n",
      "        ],\n",
      "        [\n",
      "            \"duration_usec\",\n",
      "            64\n",
      "        ],\n",
      "        [\n",
      "            \"total_pkts\",\n",
      "            16\n",
      "        ],\n",
      "        [\n",
      "            \"total_urg\",\n",
      "            16\n",
      "        ]\n",
      "    ],\n",
      "    \"total_out_bitwidth\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# let's load the dataset_metadata.json config parameters\n",
    "with open(join(BUILD_DIR_P1, \"dataset_metadata.json\"), \"r\") as fp:\n",
    "    dataset_metadata = json.load(fp)\n",
    "\n",
    "# extract feature names from metadata\n",
    "feature_columns = [x[0] for x in dataset_metadata[\"ordering\"]]\n",
    "\n",
    "test_set_fpath = join(DATASET_DIR, \"cicids2017-split.test.csv\")\n",
    "dataset = CICIDS2017_PerPacket(test_set_fpath)\n",
    "test_set, _ = dataset.get_binarized_dataset(feature_columns)\n",
    "\n",
    "# Batch size to use for inference\n",
    "batch_size = 1000\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f99a9ea8-54b7-4011-9f88-b3e5d1bd72d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's declare the model architecture -- note that this must be identical to the \n",
    "# model used in part one, or else there may be errors loading in the trained weights\n",
    "input_size = dataset_metadata[\"total_in_bitwidth\"]\n",
    "hidden1 = 32\n",
    "hidden2 = 32\n",
    "weight_bit_width = 2\n",
    "act_bit_width = 2\n",
    "num_classes = dataset_metadata[\"total_out_bitwidth\"]\n",
    "\n",
    "model = nn.Sequential(\n",
    "      QuantLinear(input_size, hidden1, bias=True, weight_bit_width=weight_bit_width),\n",
    "      nn.BatchNorm1d(hidden1),\n",
    "      nn.Dropout(0.5),\n",
    "      QuantReLU(bit_width=act_bit_width),\n",
    "      QuantLinear(hidden1, hidden2, bias=True, weight_bit_width=weight_bit_width),\n",
    "      nn.BatchNorm1d(hidden2),\n",
    "      nn.Dropout(0.5),\n",
    "      QuantReLU(bit_width=act_bit_width),\n",
    "      QuantLinear(hidden2, num_classes, bias=True, weight_bit_width=weight_bit_width)\n",
    ")\n",
    "\n",
    "# Make sure the model is on CPU before loading a pretrained state_dict\n",
    "model = model.cpu()\n",
    "\n",
    "# Load pretrained weights\n",
    "trained_state_dict = torch.load(join(BUILD_DIR_P1, \"trained_model.pth\"))\n",
    "model.load_state_dict(trained_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9dc725-71d1-424b-9b11-1e4fef4f744a",
   "metadata": {},
   "source": [
    "Now that the model is loaded with our pre-trained weights, let's verify that it delivers the same test accuracy as observed in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530e8edf-271e-40b9-8526-11393f54099d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:1255: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1758.)\n",
      "  return super(Tensor, self).rename(names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model test accuracy = 85.1426%\n"
     ]
    }
   ],
   "source": [
    "# Move model to target device and run inference on test set\n",
    "model.to(device)\n",
    "print(f\"Trained model test accuracy = {100*test(model, test_loader, device):.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c9cdf1-0704-4b97-b4a3-ce2d4ba841ae",
   "metadata": {},
   "source": [
    "### Network Surgery\n",
    "\n",
    "Often, it is desirable to make some changes to our trained network prior to generating FPGA RTL using FINN. This step is known in general as \"network surgery\". This step depends on the model and is not generally necessary, but in this case we want to make a couple of changes to get better results with FINN.\n",
    "\n",
    "We start by first moving the model to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a1ed2fd-c0e8-43b8-b0df-bb0f8cc50b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to CPU before surgery\n",
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c253112-f950-44e0-93f2-723fe6a5e490",
   "metadata": {},
   "source": [
    "One common surgery step is to pad input vector to a byte-aligned number of bits, i.e., multiple of 8b. For example, in [this notebook](https://github.com/Xilinx/finn/blob/v0.10/notebooks/end2end_example/cybersecurity/1-train-mlp-with-brevitas.ipynb), the input vector is 593b wide, and is padded to 600b to make the folding (parallelization) for the first layer simpler. The padding is done with zero values, and subsequent inference requests are made with extra 7b of zero-padding to each input. The first layer weight matrix is also affected, and zero weights must be added to the matrix. To see how that can be done, please refer to the \"Network Surgery\" section in [this notebook](https://github.com/Xilinx/finn/blob/v0.10/notebooks/end2end_example/cybersecurity/1-train-mlp-with-brevitas.ipynb). **In this example notebook, our inputs are already byte-aligned, so we skip this step.**\n",
    "\n",
    "We start by first making a copy of the original software model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2c45916-6529-41a2-bad1-5b5f6a113545",
   "metadata": {},
   "outputs": [],
   "source": [
    "surgery_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a0ace-3f81-4188-8927-eb99122c4049",
   "metadata": {},
   "source": [
    "Next, we'll modify the expected input/output ranges. In FINN, we prefer to work with bipolar {-1, +1} instead of binary {0, 1} values. To achieve this, we'll create a \"wrapper\" model that handles the pre/postprocessing as follows:\n",
    "\n",
    "* on the input side, we'll pre-process by (x + 1) / 2 in order to map incoming {-1, +1} inputs to {0, 1} ones which the trained network is used to. Since we're just multiplying/adding a scalar, these operations can be [*streamlined*](https://finn.readthedocs.io/en/latest/nw_prep.html#streamlining-transformations) by FINN and implemented with no extra cost.\n",
    "\n",
    "* on the output side, we'll add a binary quantizer which maps everthing below 0 to -1 and everything above 0 to +1. This is essentially the same behavior as the sigmoid we used earlier, except the outputs are bipolar instead of binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d972fb88-aac9-432c-9793-664306bbe351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExportModel(\n",
       "  (pretrained): Sequential(\n",
       "    (0): QuantLinear(\n",
       "      in_features=128, out_features=32, bias=True\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (output_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (weight_quant): WeightQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClampSte()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "          )\n",
       "          (scaling_impl): StatsFromParameterScaling(\n",
       "            (parameter_list_stats): _ParameterListStats(\n",
       "              (first_tracked_param): _ViewParameterWrapper(\n",
       "                (view_shape_impl): OverTensorView()\n",
       "              )\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsMax()\n",
       "              )\n",
       "            )\n",
       "            (stats_scaling_impl): _StatsScaling(\n",
       "              (affine_rescaling): Identity()\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_scaling_pre): Identity()\n",
       "            )\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bias_quant): BiasQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): QuantReLU(\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (act_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "          (activation_impl): ReLU()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClamp()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "              (stats_input_view_shape_impl): OverTensorView()\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsPercentile()\n",
       "              )\n",
       "              (restrict_scaling): _RestrictValue(\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (clamp_scaling): _ClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "              )\n",
       "              (restrict_inplace_preprocess): Identity()\n",
       "              (restrict_preprocess): Identity()\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): QuantLinear(\n",
       "      in_features=32, out_features=32, bias=True\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (output_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (weight_quant): WeightQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClampSte()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "          )\n",
       "          (scaling_impl): StatsFromParameterScaling(\n",
       "            (parameter_list_stats): _ParameterListStats(\n",
       "              (first_tracked_param): _ViewParameterWrapper(\n",
       "                (view_shape_impl): OverTensorView()\n",
       "              )\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsMax()\n",
       "              )\n",
       "            )\n",
       "            (stats_scaling_impl): _StatsScaling(\n",
       "              (affine_rescaling): Identity()\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_scaling_pre): Identity()\n",
       "            )\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bias_quant): BiasQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Dropout(p=0.5, inplace=False)\n",
       "    (7): QuantReLU(\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (act_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "          (activation_impl): ReLU()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClamp()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "              (stats_input_view_shape_impl): OverTensorView()\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsPercentile()\n",
       "              )\n",
       "              (restrict_scaling): _RestrictValue(\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (clamp_scaling): _ClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "              )\n",
       "              (restrict_inplace_preprocess): Identity()\n",
       "              (restrict_preprocess): Identity()\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): QuantLinear(\n",
       "      in_features=32, out_features=1, bias=True\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (output_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (weight_quant): WeightQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClampSte()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "          )\n",
       "          (scaling_impl): StatsFromParameterScaling(\n",
       "            (parameter_list_stats): _ParameterListStats(\n",
       "              (first_tracked_param): _ViewParameterWrapper(\n",
       "                (view_shape_impl): OverTensorView()\n",
       "              )\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsMax()\n",
       "              )\n",
       "            )\n",
       "            (stats_scaling_impl): _StatsScaling(\n",
       "              (affine_rescaling): Identity()\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_scaling_pre): Identity()\n",
       "            )\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bias_quant): BiasQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qnt_output): QuantIdentity(\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (act_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "        (activation_impl): Identity()\n",
       "        (tensor_quant): ClampedBinaryQuant(\n",
       "          (scaling_impl): ConstScaling(\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (value): StatelessBuffer()\n",
       "          )\n",
       "          (bit_width): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "          (zero_point): StatelessBuffer()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (tensor_clamp_impl): TensorClamp()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ExportModel(nn.Module):\n",
    "    def __init__(self, my_pretrained_model):\n",
    "        super(ExportModel, self).__init__()\n",
    "        self.pretrained = my_pretrained_model\n",
    "        self.qnt_output = QuantIdentity(\n",
    "            quant_type='binary', \n",
    "            scaling_impl_type='const',\n",
    "            bit_width=1, min_val=-1.0, max_val=1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # assume x contains bipolar {-1,1} elems\n",
    "        # shift from {-1,1} -> {0,1} since that is the\n",
    "        # input range for the trained network\n",
    "        x = (x + torch.tensor([1.0]).to(x.device)) / 2.0  \n",
    "        out_original = self.pretrained(x)\n",
    "        out_final = self.qnt_output(out_original)   # output as {-1,1}     \n",
    "        return out_final\n",
    "\n",
    "model_for_export = ExportModel(surgery_model)\n",
    "model_for_export.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da839d-89eb-4038-bf2b-aa87f4caa631",
   "metadata": {},
   "source": [
    "We can run inference with our test set using the same helper `test()` function used before, except now we can pass a `bipolar=True` flag to the arguments to indicate that our export model now accepts bipolar inputs and outputs bipolar outputs. There should be no change in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "885fef25-33e2-47b1-bf66-2891e3958264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bipolar export model test accuracy = 85.1426%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Bipolar export model test accuracy = {100*test(model_for_export, test_loader, device, bipolar=True):.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b95ad6-85cd-479d-8b2e-93491078074d",
   "metadata": {},
   "source": [
    "### Verification of bipolar model\n",
    "\n",
    "We can go a step further and verify that each output from this model matches the expected output from our original trained Brevitas model. We provide a `verify()` method to simplify this verification step in software. Note that we only take a subset of the test set for doing the verification for faster completion. The terms in a confusion matrix -- true-positive (`tp`), true-negative (`tn`), false-positive (`fp`), and false-negative (`fn`) -- are also printed so we can be confident that both positive and negative classes were being predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6c70fec-5f65-4a9f-8301-bf04298f1aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 1000 nok 0 (tp=367, tn=633, fp=0, fn=0): 100%|███████████████| 1000/1000 [00:04<00:00, 232.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mModel output matches with reference Brevitas software model output!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_verif = 1000\n",
    "verif_tensors = test_set.tensors[0][:num_verif]\n",
    "if verify(model_for_export, model, verif_tensors, device, bipolar=True):\n",
    "    print(f\"{bcolors.OKGREEN}Model output matches with reference Brevitas software model output!{bcolors.ENDC}\")\n",
    "else:\n",
    "    print(f\"{bcolors.FAIL}Model output differs from reference Brevitas software model output! Something went wrong...{bcolors.ENDC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03a6ae0-c7f5-4e44-9717-5c0ee709ade8",
   "metadata": {},
   "source": [
    "### Exporting the model\n",
    "\n",
    "Now for the final step in this notebook: exporting the model into an QONNX format.\n",
    "\n",
    "[ONNX](https://onnx.ai/) is an open format built to represent machine learning models, and the FINN compiler expects an ONNX model as input. We'll now export our network into ONNX to be imported and used in FINN for the next notebooks. Note that the particular ONNX representation used for FINN differs from standard ONNX, you can read more about this [here](https://finn.readthedocs.io/en/latest/internals.html#intermediate-representation-finn-onnx).\n",
    "\n",
    "You can see below how we export a trained network in Brevitas into a FINN-compatible ONNX representation (QONNX). QONNX is the format we can export from Brevitas, to feed it into the FINN compiler, we will need to make a conversion to the FINN-ONNX format which is the intermediate representation the compiler works on. The conversion of the FINN-ONNX format is a FINN compiler transformation and to be able to apply it to our model, we will need to wrap it into [ModelWrapper](https://finn.readthedocs.io/en/latest/internals.html#modelwrapper). This is a wrapper around the ONNX model which provides several helper functions to make it easier to work with the model. Then we can call the conversion function to obtain the model in FINN-ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80d42982-d2bd-43f6-a361-1975a1257d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sids/workspace/project_find_ml/finn/deps/qonnx/src/qonnx/transformation/gemm_to_matmul.py:57: UserWarning: The GemmToMatMul transformation only offers explicit support for version 9 of the Gemm node, but the ONNX version of the supplied model is 14. Thus the transformation may fail or return incomplete results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /home/sids/workspace/project_find_ml/finn/notebooks/end2end_example/ddos-anomaly-detector/build/part_02/ready-for-finn.onnx\n"
     ]
    }
   ],
   "source": [
    "# declare path to output ONNX file to save to\n",
    "model_for_export_fpath = join(BUILD_DIR, \"ready-for-finn.onnx\")\n",
    "\n",
    "# Start the export process\n",
    "input_shape = (1, dataset_metadata[\"total_in_bitwidth\"])\n",
    "\n",
    "# create a QuantTensor instance to mark input as bipolar during export\n",
    "input_a = np.random.randint(0, 1, size=input_shape).astype(np.float32)\n",
    "input_a = 2 * input_a - 1\n",
    "scale = 1.0\n",
    "input_t = torch.from_numpy(input_a * scale)\n",
    "\n",
    "# Move to CPU before export\n",
    "model_for_export.cpu()\n",
    "\n",
    "# Export to ONNX\n",
    "export_qonnx(\n",
    "    model_for_export,\n",
    "    export_path=model_for_export_fpath,\n",
    "    input_t=input_t\n",
    ")\n",
    "\n",
    "# clean-up\n",
    "qonnx_cleanup(model_for_export_fpath, out_file=model_for_export_fpath)\n",
    "\n",
    "# ModelWrapper\n",
    "model_for_export = ModelWrapper(model_for_export_fpath)\n",
    "\n",
    "# Setting the input datatype explicitly because it doesn't get derived from the export function\n",
    "model_for_export.set_tensor_datatype(model_for_export.graph.input[0].name, DataType[\"BIPOLAR\"])\n",
    "model_for_export = model_for_export.transform(ConvertQONNXtoFINN())\n",
    "model_for_export.save(model_for_export_fpath)\n",
    "\n",
    "print(f\"Model saved to {model_for_export_fpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1908dc7-d419-404c-8dd2-730b8b3e0484",
   "metadata": {},
   "source": [
    "### Viewing the ONNX Model in Netron\n",
    "\n",
    "We can visualize what the exported ONNX model looks like using [Netron](https://github.com/lutzroeder/netron), which is a visualizer for neural networks and allows interactive investigation of network properties. For example, you can click on the individual nodes and view the properties. Particular things of note:\n",
    "\n",
    "* The input tensor `global_in` is annotated with `finn_datatype = BIPOLAR`\n",
    "* The input preprocessing (x + 1) / 2 is exported as part of the network (initial `Add` and `Div` layers)\n",
    "* Brevitas `QuantLinear` layers are exported to ONNX as `MatMul`. We've exported the padded version; shape of the first MatMul node's weight parameter is 128x32 (`num_inputs` x `size_of_first_layer`).\n",
    "* The weight parameters (second inputs) for MatMul nodes are annotated with `finn_datatype = INT2`\n",
    "* The quantized activations are exported as `MultiThreshold` nodes with `module = qonnx.custom_op.general`\n",
    "* There's a final `MultiThreshold` node with threshold = 0 (second input) to produce the final bipolar output (this is the `qnt_output` from `ExportModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b05ce2c6-4700-4a53-8089-4856b5537cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/home/sids/workspace/project_find_ml/finn/notebooks/end2end_example/ddos-anomaly-detector/build/part_02/ready-for-finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f42475df190>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(model_for_export_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd20e8b-ea74-4fde-b377-66c96bd27f93",
   "metadata": {},
   "source": [
    "### Verification of ONNX model\n",
    "\n",
    "We can also verify that the exported ONNX model to make sure that we have not lost any functional correctness going through the steps above. Since this is an ONNX model now, we need ONNX runtime to execute the graph to produce the inference output.\n",
    "\n",
    "Before running the verification, we need to prepare our FINN-ONNX model. In particular, all the intermediate tensors need to have statically defined shapes. To do this, we apply some graph transformations to the model like a kind of \"tidy-up\" to make it easier to process. \n",
    "\n",
    "**Graph transformations in FINN:** The whole FINN compiler is built around the idea of transformations, which gradually transform the model into a synthesizable hardware description. Although FINN offers functionality that automatically calls a standard sequence of transformations (covered in the next notebook), you can also manually call individual transformations (like we do here), as well as adding your own transformations, to create custom flows. You can read more about these transformations in [this notebook](https://github.com/Xilinx/finn/blob/v0.10/notebooks/end2end_example/bnn-pynq/tfc_end2end_example.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5abc12bc-f1a5-4b26-9f9a-3e59448a59bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from file\n",
    "model_for_verif = ModelWrapper(model_for_export_fpath)\n",
    "\n",
    "# apply the tidy-up transformations\n",
    "model_for_verif = model_for_verif.transform(InferShapes())\n",
    "model_for_verif = model_for_verif.transform(FoldConstants())\n",
    "model_for_verif = model_for_verif.transform(GiveUniqueNodeNames())\n",
    "model_for_verif = model_for_verif.transform(GiveReadableTensorNames())\n",
    "model_for_verif = model_for_verif.transform(InferDataTypes())\n",
    "model_for_verif = model_for_verif.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "# save the verification model\n",
    "model_for_verif_fpath = join(BUILD_DIR, \"ready-for-finn-verif.onnx\")\n",
    "model_for_verif.save(model_for_verif_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461ecf29-3b8c-437d-8e5a-761a8ef1e273",
   "metadata": {},
   "source": [
    "**Would the FINN compiler still work if we didn't do this?** The compilation step in the next notebook applies these transformations internally and would work fine, but we're going to use FINN's verification capabilities below and these require the tidy-up transformations. Note that in FINN v0.10 release, `qonnx_cleanup()` already handles most, if not all, of these transformations, and they may not be strictly needed for the verification step. However, running them again doesn't produce any unwanted side-effects, and can be thought of as a sanity-check to handle edge-cases where some annotations were missing. This is especially important for the `InferShapes()` and `InferDataTypes()` transformations, as the ONNX runtime needs to know the shape and data types of tensors in order to process the graph during inference.\n",
    "\n",
    "We can view our \"verification\" model after the transformations. Note that all intermediate tensors must have their shapes specified (indicated by numbers next to the arrows going between layers). Additionally, the `InferDataTypes()` transformation has propagated quantization annotations to the outputs of `MultiThreshold` layers (expand by clicking the + next to the name of the tensor to see the quantization annotation) and the final output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9126a08b-5a0c-477d-83d4-dbfb91aaba8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/home/sids/workspace/project_find_ml/finn/notebooks/end2end_example/ddos-anomaly-detector/build/part_02/ready-for-finn-verif.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f4242c9e3b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(model_for_verif_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a31f69-167c-475f-8a65-bd9e19d0dc6e",
   "metadata": {},
   "source": [
    "Similar to the verification step above, we have provided a helper verification function, `verify_onnx()` that verifies that the outputs of the ONNX model match that from the golden Brevitas reference model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d62e5363-9118-43e1-812e-60f176330cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 100 nok 0 (tp=35, tn=65, fp=0, fn=0): 100%|█████████████████████| 100/100 [00:05<00:00, 17.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mFINN-ONNX model output matches with reference Brevitas software model output!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_verif = 100\n",
    "verif_tensors = test_set.tensors[0][:num_verif]\n",
    "if verify_onnx(model_for_verif, model, verif_tensors, device):\n",
    "    print(f\"{bcolors.OKGREEN}FINN-ONNX model output matches with reference Brevitas software model output!{bcolors.ENDC}\")\n",
    "else:\n",
    "    print(f\"{bcolors.FAIL}FINN-ONNX model output differs from reference Brevitas software model output! Something went wrong...{bcolors.ENDC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e01a34-e93e-48b2-8785-2ff01b6ae4cc",
   "metadata": {},
   "source": [
    "That's it! If all the verification steps passed, we are ready to move on to the next notebook in this series: [Part 3](./3-build.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
