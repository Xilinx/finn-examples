{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf1903e",
   "metadata": {},
   "source": [
    "# Initialize the accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f012d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn_examples import models\n",
    "print(list(filter(lambda x: \"unsw_nb15\" in x, dir(models))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8446d7",
   "metadata": {},
   "source": [
    "Specify a batch size & create the FINN overlay. Note that the batch size must divide 82000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7e32e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "accel = models.mlp_w2a2_unsw_nb15()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912943c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Expected input shape and datatype: %s %s\" % (str(accel.ishape_normal()), str(accel.idt())))\n",
    "print(\"Expected output shape and datatype: %s %s\" % (str(accel.oshape_normal()), str(accel.odt())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e899e",
   "metadata": {},
   "source": [
    "# Load the binarized UNSW-NB15 test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -nc -O unsw_nb15_binarized.npz https://zenodo.org/record/4519767/files/unsw_nb15_binarized.npz?download=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc2ae98",
   "metadata": {},
   "source": [
    "Note that the generated design expects inputs of length 600. As explained in the [end-to-end notebook](https://github.com/Xilinx/finn/blob/main/notebooks/end2end_example/cybersecurity/1-train-mlp-with-brevitas.ipynb) in the FINN repository, padding the input data from length 593 to 600 enables SIMD parallelization for the first layer.\n",
    "Thus, we'll have to pad our dataset before feeding it to the accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6631508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_unsw_nb15_test_batches(bsize):\n",
    "    unsw_nb15_data = np.load(\"unsw_nb15_binarized.npz\")[\"test\"][:82000]\n",
    "    test_imgs = unsw_nb15_data[:, :-1]\n",
    "    test_imgs = np.pad(test_imgs, [(0, 0), [0, 7]], mode=\"constant\")\n",
    "    test_labels = unsw_nb15_data[:, -1]\n",
    "    n_batches = int(test_imgs.shape[0] / bsize)\n",
    "    test_imgs = test_imgs.reshape(n_batches, bsize, -1)\n",
    "    test_labels = test_labels.reshape(n_batches, bsize)\n",
    "    return (test_imgs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dbb1d3",
   "metadata": {},
   "source": [
    "# Classify a single attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92e1b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_imgs, test_labels) = make_unsw_nb15_test_batches(bsize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f308f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_single = test_imgs[-1]\n",
    "test_single_label = test_labels[-1].astype(np.float32)\n",
    "\n",
    "print(\"Expected label is: %d (%s data)\" % (test_single_label, (lambda x: \"normal\" if x==0 else \"abnormal\")(test_single_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e39fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the accelerator expects binary input data presented in bipolar form (i.e. {-1, 1})\n",
    "accel_in = 2 * test_single - 1\n",
    "accel_out = accel.execute(accel_in)\n",
    "# To convert back to the original label (i.e. {0, 1}), we'll have to map the bipolar output to binary\n",
    "accel_out_binary = (accel_out + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c273dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Returned label is: %d (%s data)\" % (accel_out_binary, (lambda x: \"normal\" if x==0 else \"abnormal\")(accel_out_binary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149e06f",
   "metadata": {},
   "source": [
    "# Validate accuracy on 82000 (out of 82332) records from UNSW-NB15 test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b841f2",
   "metadata": {},
   "source": [
    "To increase the throughput, let's increase the batch size. Note that the FINN accelerator operates on a batch size of 1, but to fill the compute pipeline, we'll copy a greater chunk of the test set to the device buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51e3e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "accel.batch_size = batch_size\n",
    "(test_imgs, test_labels) = make_unsw_nb15_test_batches(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c88f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = 0\n",
    "nok = 0\n",
    "n_batches = test_imgs.shape[0]\n",
    "total = batch_size*n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26358f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_batches):\n",
    "    inp = test_imgs[i].astype(np.float32)\n",
    "    exp = test_labels[i].astype(np.float32)\n",
    "    inp = 2 * inp - 1\n",
    "    exp = 2 * exp - 1\n",
    "    out = accel.execute(inp)\n",
    "    matches = np.count_nonzero(out.flatten() == exp.flatten())\n",
    "    nok += batch_size - matches\n",
    "    ok += matches\n",
    "    print(\"batch %d / %d : total OK %d NOK %d\" % (i + 1, n_batches, ok, nok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af33f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 100.0 * ok / (total)\n",
    "print(\"Final accuracy: {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d63354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation():\n",
    "    for i in range(n_batches):\n",
    "        ibuf_normal = test_imgs[i].reshape(accel.ishape_normal())\n",
    "        accel.execute(ibuf_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a7fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_validation_time = %timeit -n 1 -o run_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dde028",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%f images per second including data movement\" % (total / float(full_validation_time.best)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c0d3a",
   "metadata": {},
   "source": [
    "# More benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79491c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accel.throughput_test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ca5c916",
   "metadata": {},
   "source": [
    "The measured `throughput` of the accelerator, excluding any software and data movement overhead, is influenced by the batch size. The more we fill the compute pipeline, the higher the throughput.\n",
    "Note that the total runtime consists of the overhead of packing/unpacking the inputs/outputs to convert form numpy arrays to the bit-contiguous data representation our accelerator expectes (`pack_input`/`unpack_output`), the cost of moving data between the CPU and accelerator memories (`copy_input_data_to_device`/`copy_output_data_from_device`), as well as the accelerator's execution time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
