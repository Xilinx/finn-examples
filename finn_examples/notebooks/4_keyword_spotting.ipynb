{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Validating network accuracy\n",
    "In this first part we will be looking at the overall accuracy of the network.\n",
    "\n",
    "The keyword spotting (KWS) network was trained on the Google Speech Commands v2 dataset, as published here: https://arxiv.org/abs/1804.03209\n",
    "\n",
    "We then used a feature extraction technique called Mel Frequency Cepstral Coefficients or MFCC for short.\n",
    "This method turns audio waveforms into 2D images with one channel. Similar to the one shown below:\n",
    "\n",
    "<img src=\"images/mfcc_py.png\">\n",
    "\n",
    "A more in-depth explanation of MFCC features can be found on wikipedia: https://en.wikipedia.org/wiki/Mel-frequency_cepstrum\n",
    "\n",
    "For this concrete case we used the python library [python_speech_features](https://github.com/jameslyons/python_speech_features) to produce these features.\n",
    "\n",
    "During the training of the KWS network we produce the MFCC features for the training and validation set and then quantize the inputs to the network to eight bit.\n",
    "We will load the pre-processed and quantized validation dataset in the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed Google Speech Commands v2 validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources as pk\n",
    "import numpy as np\n",
    "\n",
    "input_npy = pk.resource_filename(\"finn_examples\", \"data/python_speech_preprocessing_all_validation_KWS_data_inputs_len_10102.npy\")\n",
    "golden_out_npy = pk.resource_filename(\"finn_examples\", \"data/python_speech_preprocessing_all_validation_KWS_data_outputs_len_10102.npy\")\n",
    "\n",
    "input_data = np.load(input_npy)\n",
    "golden_out_data = np.load(golden_out_npy)\n",
    "num_samples = input_data.shape[0]\n",
    "\n",
    "print(\"Input data shape: \" + str(input_data.shape))\n",
    "print(\"Label shape: \" + str(golden_out_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn_examples import models\n",
    "print(list(filter(lambda x: \"kws\" in x, dir(models))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accel = models.kws_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Expected input shape and datatype: %s %s\" % (str(accel.ishape_normal()), str(accel.idt())))\n",
    "print(\"Expected output shape and datatype: %s %s\" % (str(accel.oshape_normal()), str(accel.odt())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run validation on the FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accel.batch_size = num_samples\n",
    "accel_out_data = accel.execute(input_data)\n",
    "\n",
    "print(\"Accelerator output shape: \" + str(accel_out_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.unique(accel_out_data.flatten() == golden_out_data.flatten(), return_counts=True)\n",
    "print(\"Correctly predicted: %d / %d \" % (score[1][1], num_samples))\n",
    "print(\"Incorrectly predicted: %d / %d \" % (score[1][0], num_samples))\n",
    "print(\"Accuracy: %f%%\" % (100.0 * score[1][1] / num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here you should see an accuracy of about 88.76 %."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assessing network throughput\n",
    "\n",
    "Now we will take a look at how fast the FPGA can process the whole validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Using a naive timing benchmark from the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation():\n",
    "    accel_out_data = accel.execute(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_validation_time = %timeit -n 5 -o run_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{(num_samples / float(full_validation_time.best)):.0f} samples per second including data movement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the result of over 140 thousand inferences per second is already very good, this naive benchmark\n",
    "also includes data movement from and to the FPGA and it is dificult to assess how much time is spent on\n",
    "which part of running the FINN accelerator.\n",
    "\n",
    "### Using the built-in performance benchmark\n",
    "\n",
    "To measure the performance of indivudual components of the PYNQ stack and the FINN accelerator on the FPGA,\n",
    "FINN comes with a built-in benchmark. This benchmark computes the throughput of the FINN accelerator as seen on the FPGA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accel.throughput_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying .wav files with the KWS network\n",
    "\n",
    "Now we are going to look at how to classify raw .wav files with the KWS network. We include some sample files with finn-examples, but in theory you can also classify your own recordings. To do this one can simply modify where to load the .wav file from. However, one needs to make sure that the file is shorter than one second.\n",
    "\n",
    "First we will install python_speech_features, to generate the MFCC features later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install python_speech_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_speech_features import mfcc\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import scipy.io.wavfile as wav\n",
    "from scipy.signal.windows import hann\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing parameters\n",
    "tf_desired_samples = 16000\n",
    "tf_window_size_samples = 480\n",
    "tf_sample_rate = 16000\n",
    "tf_window_size_ms = 30.\n",
    "tf_window_stride_ms = 20.\n",
    "tf_dct_coefficient_count = 10\n",
    "\n",
    "# Dataset parameter\n",
    "tf_dataset_labels = ['down', 'go', 'left', 'no', 'off', 'on', 'right', 'stop', 'up', 'yes', 'SILENCE', 'UNKNOWN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience functions\n",
    "def py_speech_preprocessing(resampled_data, sample_rate,\n",
    "                            tf_desired_samples=tf_desired_samples, \n",
    "                            tf_window_size_samples=tf_window_size_samples, \n",
    "                            tf_sample_rate=tf_sample_rate, \n",
    "                            tf_window_size_ms=tf_window_size_ms, \n",
    "                            tf_dct_coefficient_count=tf_dct_coefficient_count):\n",
    "    # Resample\n",
    "    num_target_samples = round(tf_sample_rate / sample_rate * len(raw_signal))\n",
    "    resampled_data = signal.resample(raw_signal, num_target_samples)\n",
    "    # Rescale\n",
    "    rescaled_data = resampled_data / np.max(resampled_data)\n",
    "    # Pad\n",
    "    padded_data = np.pad(rescaled_data, [[0, tf_desired_samples - rescaled_data.shape[-1]]], mode=\"constant\")\n",
    "    # Calculate MFCC features\n",
    "    nfft = int(2**np.ceil(np.log2(tf_window_size_samples)))\n",
    "    mfcc_feat_py = mfcc(padded_data, tf_sample_rate, \n",
    "                     winlen = tf_window_size_ms / 1000.,\n",
    "                     winstep = tf_window_stride_ms / 1000.,\n",
    "                     numcep = tf_dct_coefficient_count,\n",
    "                     nfilt = 40,\n",
    "                     nfft = nfft,\n",
    "                     lowfreq = 20.0,\n",
    "                     highfreq = 4000.0,\n",
    "                     winfunc=hann,\n",
    "                     appendEnergy=False,\n",
    "                     preemph=0.,\n",
    "                     ceplifter=0.,\n",
    "                    )\n",
    "    # Cut and transpose MFCC features\n",
    "    mfcc_feat_py = mfcc_feat_py[:-1,:].T\n",
    "    \n",
    "    return mfcc_feat_py\n",
    "\n",
    "\n",
    "def quantize_input(mfcc_feat_py):\n",
    "    # Scaling\n",
    "    quant_mfcc_feat = (mfcc_feat_py / 0.8298503756523132)\n",
    "    # Clamping & rounding\n",
    "    quant_mfcc_feat = np.where(quant_mfcc_feat > 127., 127., quant_mfcc_feat)\n",
    "    quant_mfcc_feat = np.where(quant_mfcc_feat < -127., -127., quant_mfcc_feat)\n",
    "    quant_mfcc_feat = np.round(quant_mfcc_feat)\n",
    "    quant_mfcc_feat = quant_mfcc_feat.astype(np.int8).reshape((1,490))\n",
    "    \n",
    "    return quant_mfcc_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and pre-processing the audio file\n",
    "\n",
    "The following sample files are included with finn-examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sample files\n",
    "audio_samples_folder = pk.resource_filename(\"finn_examples\", \"data/audio_samples/\")\n",
    "!ls $audio_samples_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change sample_path variable in the line below to load your own .wav file or to load a different sample file.\n",
    "Make sure that the file is shorter than one second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_path = f\"{audio_samples_folder}audio_sample_yes.wav\"\n",
    "IPython.display.Audio(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the audio wave file\n",
    "rate, raw_signal = wav.read(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pre-processing\n",
    "mfcc_feat_py = py_speech_preprocessing(raw_signal, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the MFCC features\n",
    "plt.matshow(mfcc_feat_py)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize MFCC features\n",
    "quant_mfcc_feat = quantize_input(mfcc_feat_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying the pre-processed audio on the FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on the FPGA\n",
    "accel.batch_size = 1\n",
    "res_acc = accel.execute(quant_mfcc_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_label = tf_dataset_labels[res_acc[0,0].astype(int)]\n",
    "print(f\"The audio file was classified as: {res_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well you should see the audio file being classified correctly.\n",
    "\n",
    "However,you may notice that the 'down' sample is wrongly classified as 'go'. This is likely a side effect of the KWS network being a very simple architecture. This means that the network works better for some classes than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_classes = ['down', 'go', 'left', 'no', 'off', 'on', 'right', 'stop', 'up', 'yes']\n",
    "for sample_class in sample_classes:\n",
    "    rate, raw_signal = wav.read(f\"{audio_samples_folder}audio_sample_{sample_class}.wav\")\n",
    "    mfcc_feat_py = py_speech_preprocessing(raw_signal, rate)\n",
    "    quant_mfcc_feat = quantize_input(mfcc_feat_py)\n",
    "    accel.batch_size = 1\n",
    "    res_acc = accel.execute(quant_mfcc_feat)\n",
    "    res_label = tf_dataset_labels[res_acc[0,0].astype(int)]\n",
    "    print(f\"The audio file for {sample_class} was classified as: {res_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
